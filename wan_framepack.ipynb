{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda3/envs/FramePack/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/miniconda3/envs/FramePack/lib/python3.10/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Tuple, Optional, List, Union\n",
    "import os\n",
    "import math\n",
    "import traceback\n",
    "import einops\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from diffusers import WanPipeline\n",
    "from transformers import AutoTokenizer, UMT5EncoderModel\n",
    "from diffusers.models.transformers.transformer_wan import WanTransformer3DModel,register_to_config\n",
    "from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n",
    "from diffusers_helper.utils import save_bcthw_as_mp4, crop_or_pad_yield_mask, soft_append_bcthw, resize_and_center_crop, generate_timestamp\n",
    "from diffusers_helper.memory import cpu, gpu, get_cuda_free_memory_gb, move_model_to_device_with_memory_preservation, offload_model_from_device_for_memory_preservation, fake_diffusers_current_device, unload_complete_models, load_model_as_complete\n",
    "from diffusers_helper.bucket_tools import find_nearest_bucket\n",
    "from diffusers.schedulers import UniPCMultistepScheduler\n",
    "\n",
    "import torch.utils.checkpoint\n",
    "\n",
    "from diffusers.utils.accelerate_utils import apply_forward_hook\n",
    "from diffusers.models.modeling_outputs import AutoencoderKLOutput\n",
    "from diffusers.models.autoencoders.vae import DecoderOutput, DiagonalGaussianDistribution\n",
    "from diffusers.models.autoencoders.autoencoder_kl_wan import WanCausalConv3d, WanEncoder3d, WanDecoder3d, AutoencoderKLWan as OriginalAutoencoderKLWan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'Wan-AI/Wan2.1-T2V-1.3B-Diffusers'\n",
    "# model_id = '/home/tippy/.cache/huggingface/models--Wan-AI--Wan2.1-T2V-1.3B-Diffusers/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AutoencoderKLWan(OriginalAutoencoderKLWan):\n",
    "    r\"\"\"\n",
    "    A VAE model with KL loss for encoding videos into latents and decoding latent representations into videos.\n",
    "    Introduced in [Wan 2.1].\n",
    "\n",
    "    This model inherits from [`ModelMixin`]. Check the superclass documentation for it's generic methods implemented\n",
    "    for all models (such as downloading or saving).\n",
    "    \"\"\"\n",
    "\n",
    "    _supports_gradient_checkpointing = False\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_dim: int = 96,\n",
    "        z_dim: int = 16,\n",
    "        dim_mult: Tuple[int] = [1, 2, 4, 4],\n",
    "        num_res_blocks: int = 2,\n",
    "        attn_scales: List[float] = [],\n",
    "        temperal_downsample: List[bool] = [False, True, True],\n",
    "        dropout: float = 0.0,\n",
    "        latents_mean: List[float] = [\n",
    "            -0.7571,\n",
    "            -0.7089,\n",
    "            -0.9113,\n",
    "            0.1075,\n",
    "            -0.1745,\n",
    "            0.9653,\n",
    "            -0.1517,\n",
    "            1.5508,\n",
    "            0.4134,\n",
    "            -0.0715,\n",
    "            0.5517,\n",
    "            -0.3632,\n",
    "            -0.1922,\n",
    "            -0.9497,\n",
    "            0.2503,\n",
    "            -0.2921,\n",
    "        ],\n",
    "        latents_std: List[float] = [\n",
    "            2.8184,\n",
    "            1.4541,\n",
    "            2.3275,\n",
    "            2.6558,\n",
    "            1.2196,\n",
    "            1.7708,\n",
    "            2.6052,\n",
    "            2.0743,\n",
    "            3.2687,\n",
    "            2.1526,\n",
    "            2.8652,\n",
    "            1.5579,\n",
    "            1.6382,\n",
    "            1.1253,\n",
    "            2.8251,\n",
    "            1.9160,\n",
    "        ],\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.z_dim = z_dim\n",
    "        self.temperal_downsample = temperal_downsample\n",
    "        self.temperal_upsample = temperal_downsample[::-1]\n",
    "\n",
    "        self.encoder = WanEncoder3d(\n",
    "            base_dim, z_dim * 2, dim_mult, num_res_blocks, attn_scales, self.temperal_downsample, dropout\n",
    "        )\n",
    "        self.quant_conv = WanCausalConv3d(z_dim * 2, z_dim * 2, 1)\n",
    "        self.post_quant_conv = WanCausalConv3d(z_dim, z_dim, 1)\n",
    "\n",
    "        self.decoder = WanDecoder3d(\n",
    "            base_dim, z_dim, dim_mult, num_res_blocks, attn_scales, self.temperal_upsample, dropout\n",
    "        )\n",
    "\n",
    "        self.spatial_compression_ratio = 2 ** len(self.temperal_downsample)\n",
    "\n",
    "        # When decoding a batch of video latents at a time, one can save memory by slicing across the batch dimension\n",
    "        # to perform decoding of a single video latent at a time.\n",
    "        self.use_slicing = False\n",
    "\n",
    "        # When decoding spatially large video latents, the memory requirement is very high. By breaking the video latent\n",
    "        # frames spatially into smaller tiles and performing multiple forward passes for decoding, and then blending the\n",
    "        # intermediate tiles together, the memory requirement can be lowered.\n",
    "        self.use_tiling = False\n",
    "\n",
    "        # The minimal tile height and width for spatial tiling to be used\n",
    "        self.tile_sample_min_height = 256\n",
    "        self.tile_sample_min_width = 256\n",
    "\n",
    "        # The minimal distance between two spatial tiles\n",
    "        self.tile_sample_stride_height = 192\n",
    "        self.tile_sample_stride_width = 192\n",
    "\n",
    "    def enable_tiling(\n",
    "        self,\n",
    "        tile_sample_min_height: Optional[int] = None,\n",
    "        tile_sample_min_width: Optional[int] = None,\n",
    "        tile_sample_stride_height: Optional[float] = None,\n",
    "        tile_sample_stride_width: Optional[float] = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "        Enable tiled VAE decoding. When this option is enabled, the VAE will split the input tensor into tiles to\n",
    "        compute decoding and encoding in several steps. This is useful for saving a large amount of memory and to allow\n",
    "        processing larger images.\n",
    "\n",
    "        Args:\n",
    "            tile_sample_min_height (`int`, *optional*):\n",
    "                The minimum height required for a sample to be separated into tiles across the height dimension.\n",
    "            tile_sample_min_width (`int`, *optional*):\n",
    "                The minimum width required for a sample to be separated into tiles across the width dimension.\n",
    "            tile_sample_stride_height (`int`, *optional*):\n",
    "                The minimum amount of overlap between two consecutive vertical tiles. This is to ensure that there are\n",
    "                no tiling artifacts produced across the height dimension.\n",
    "            tile_sample_stride_width (`int`, *optional*):\n",
    "                The stride between two consecutive horizontal tiles. This is to ensure that there are no tiling\n",
    "                artifacts produced across the width dimension.\n",
    "        \"\"\"\n",
    "        print(\"Enabling tiled VAE decoding. This will split the input tensor into tiles to compute decoding in several steps.\")\n",
    "        self.use_tiling = True\n",
    "        self.tile_sample_min_height = tile_sample_min_height or self.tile_sample_min_height\n",
    "        self.tile_sample_min_width = tile_sample_min_width or self.tile_sample_min_width\n",
    "        self.tile_sample_stride_height = tile_sample_stride_height or self.tile_sample_stride_height\n",
    "        self.tile_sample_stride_width = tile_sample_stride_width or self.tile_sample_stride_width\n",
    "\n",
    "    def disable_tiling(self) -> None:\n",
    "        r\"\"\"\n",
    "        Disable tiled VAE decoding. If `enable_tiling` was previously enabled, this method will go back to computing\n",
    "        decoding in one step.\n",
    "        \"\"\"\n",
    "        self.use_tiling = False\n",
    "\n",
    "    def enable_slicing(self) -> None:\n",
    "        r\"\"\"\n",
    "        Enable sliced VAE decoding. When this option is enabled, the VAE will split the input tensor in slices to\n",
    "        compute decoding in several steps. This is useful to save some memory and allow larger batch sizes.\n",
    "        \"\"\"\n",
    "        self.use_slicing = True\n",
    "\n",
    "    def disable_slicing(self) -> None:\n",
    "        r\"\"\"\n",
    "        Disable sliced VAE decoding. If `enable_slicing` was previously enabled, this method will go back to computing\n",
    "        decoding in one step.\n",
    "        \"\"\"\n",
    "        self.use_slicing = False\n",
    "\n",
    "    def _encode(self, x: torch.Tensor):\n",
    "        _, _, num_frame, height, width = x.shape\n",
    "\n",
    "        if self.use_tiling and (width > self.tile_sample_min_width or height > self.tile_sample_min_height):\n",
    "            return self.tiled_encode(x)\n",
    "\n",
    "        self.clear_cache()\n",
    "        iter_ = 1 + (num_frame - 1) // 4\n",
    "        for i in range(iter_):\n",
    "            self._enc_conv_idx = [0]\n",
    "            if i == 0:\n",
    "                out = self.encoder(x[:, :, :1, :, :], feat_cache=self._enc_feat_map, feat_idx=self._enc_conv_idx)\n",
    "            else:\n",
    "                out_ = self.encoder(\n",
    "                    x[:, :, 1 + 4 * (i - 1) : 1 + 4 * i, :, :],\n",
    "                    feat_cache=self._enc_feat_map,\n",
    "                    feat_idx=self._enc_conv_idx,\n",
    "                )\n",
    "                out = torch.cat([out, out_], 2)\n",
    "\n",
    "        enc = self.quant_conv(out)\n",
    "        self.clear_cache()\n",
    "        return enc\n",
    "\n",
    "    @apply_forward_hook\n",
    "    def encode(\n",
    "        self, x: torch.Tensor, return_dict: bool = True\n",
    "    ) -> Union[AutoencoderKLOutput, Tuple[DiagonalGaussianDistribution]]:\n",
    "        r\"\"\"\n",
    "        Encode a batch of images into latents.\n",
    "\n",
    "        Args:\n",
    "            x (`torch.Tensor`): Input batch of images.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether to return a [`~models.autoencoder_kl.AutoencoderKLOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "                The latent representations of the encoded videos. If `return_dict` is True, a\n",
    "                [`~models.autoencoder_kl.AutoencoderKLOutput`] is returned, otherwise a plain `tuple` is returned.\n",
    "        \"\"\"\n",
    "        if self.use_slicing and x.shape[0] > 1:\n",
    "            encoded_slices = [self._encode(x_slice) for x_slice in x.split(1)]\n",
    "            h = torch.cat(encoded_slices)\n",
    "        else:\n",
    "            h = self._encode(x)\n",
    "        posterior = DiagonalGaussianDistribution(h)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (posterior,)\n",
    "        return AutoencoderKLOutput(latent_dist=posterior)\n",
    "\n",
    "    def _decode(self, z: torch.Tensor, return_dict: bool = True):\n",
    "        _, _, num_frame, height, width = z.shape\n",
    "        tile_latent_min_height = self.tile_sample_min_height // self.spatial_compression_ratio\n",
    "        tile_latent_min_width = self.tile_sample_min_width // self.spatial_compression_ratio\n",
    "\n",
    "        if self.use_tiling and (width > tile_latent_min_width or height > tile_latent_min_height):\n",
    "            return self.tiled_decode(z, return_dict=return_dict)\n",
    "\n",
    "        self.clear_cache()\n",
    "        x = self.post_quant_conv(z)\n",
    "        for i in range(num_frame):\n",
    "            self._conv_idx = [0]\n",
    "            if i == 0:\n",
    "                out = self.decoder(x[:, :, i : i + 1, :, :], feat_cache=self._feat_map, feat_idx=self._conv_idx)\n",
    "            else:\n",
    "                out_ = self.decoder(x[:, :, i : i + 1, :, :], feat_cache=self._feat_map, feat_idx=self._conv_idx)\n",
    "                out = torch.cat([out, out_], 2)\n",
    "\n",
    "        out = torch.clamp(out, min=-1.0, max=1.0)\n",
    "        self.clear_cache()\n",
    "        if not return_dict:\n",
    "            return (out,)\n",
    "\n",
    "        return DecoderOutput(sample=out)\n",
    "\n",
    "    @apply_forward_hook\n",
    "    def decode(self, z: torch.Tensor, return_dict: bool = True) -> Union[DecoderOutput, torch.Tensor]:\n",
    "        r\"\"\"\n",
    "        Decode a batch of images.\n",
    "\n",
    "        Args:\n",
    "            z (`torch.Tensor`): Input batch of latent vectors.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether to return a [`~models.vae.DecoderOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "            [`~models.vae.DecoderOutput`] or `tuple`:\n",
    "                If return_dict is True, a [`~models.vae.DecoderOutput`] is returned, otherwise a plain `tuple` is\n",
    "                returned.\n",
    "        \"\"\"\n",
    "        if self.use_slicing and z.shape[0] > 1:\n",
    "            decoded_slices = [self._decode(z_slice).sample for z_slice in z.split(1)]\n",
    "            decoded = torch.cat(decoded_slices)\n",
    "        else:\n",
    "            decoded = self._decode(z).sample\n",
    "\n",
    "        if not return_dict:\n",
    "            return (decoded,)\n",
    "        return DecoderOutput(sample=decoded)\n",
    "\n",
    "    def blend_v(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-2], b.shape[-2], blend_extent)\n",
    "        for y in range(blend_extent):\n",
    "            b[:, :, :, y, :] = a[:, :, :, -blend_extent + y, :] * (1 - y / blend_extent) + b[:, :, :, y, :] * (\n",
    "                y / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def blend_h(self, a: torch.Tensor, b: torch.Tensor, blend_extent: int) -> torch.Tensor:\n",
    "        blend_extent = min(a.shape[-1], b.shape[-1], blend_extent)\n",
    "        for x in range(blend_extent):\n",
    "            b[:, :, :, :, x] = a[:, :, :, :, -blend_extent + x] * (1 - x / blend_extent) + b[:, :, :, :, x] * (\n",
    "                x / blend_extent\n",
    "            )\n",
    "        return b\n",
    "\n",
    "    def tiled_encode(self, x: torch.Tensor) -> AutoencoderKLOutput:\n",
    "        r\"\"\"Encode a batch of images using a tiled encoder.\n",
    "\n",
    "        Args:\n",
    "            x (`torch.Tensor`): Input batch of videos.\n",
    "\n",
    "        Returns:\n",
    "            `torch.Tensor`:\n",
    "                The latent representation of the encoded videos.\n",
    "        \"\"\"\n",
    "        _, _, num_frames, height, width = x.shape\n",
    "        latent_height = height // self.spatial_compression_ratio\n",
    "        latent_width = width // self.spatial_compression_ratio\n",
    "\n",
    "        tile_latent_min_height = self.tile_sample_min_height // self.spatial_compression_ratio\n",
    "        tile_latent_min_width = self.tile_sample_min_width // self.spatial_compression_ratio\n",
    "        tile_latent_stride_height = self.tile_sample_stride_height // self.spatial_compression_ratio\n",
    "        tile_latent_stride_width = self.tile_sample_stride_width // self.spatial_compression_ratio\n",
    "\n",
    "        blend_height = tile_latent_min_height - tile_latent_stride_height\n",
    "        blend_width = tile_latent_min_width - tile_latent_stride_width\n",
    "\n",
    "        # Split x into overlapping tiles and encode them separately.\n",
    "        # The tiles have an overlap to avoid seams between tiles.\n",
    "        rows = []\n",
    "        for i in range(0, height, self.tile_sample_stride_height):\n",
    "            row = []\n",
    "            for j in range(0, width, self.tile_sample_stride_width):\n",
    "                self.clear_cache()\n",
    "                time = []\n",
    "                frame_range = 1 + (num_frames - 1) // 4\n",
    "                for k in range(frame_range):\n",
    "                    self._enc_conv_idx = [0]\n",
    "                    if k == 0:\n",
    "                        tile = x[:, :, :1, i : i + self.tile_sample_min_height, j : j + self.tile_sample_min_width]\n",
    "                    else:\n",
    "                        tile = x[\n",
    "                            :,\n",
    "                            :,\n",
    "                            1 + 4 * (k - 1) : 1 + 4 * k,\n",
    "                            i : i + self.tile_sample_min_height,\n",
    "                            j : j + self.tile_sample_min_width,\n",
    "                        ]\n",
    "                    tile = self.encoder(tile, feat_cache=self._enc_feat_map, feat_idx=self._enc_conv_idx)\n",
    "                    tile = self.quant_conv(tile)\n",
    "                    time.append(tile)\n",
    "                row.append(torch.cat(time, dim=2))\n",
    "            rows.append(row)\n",
    "        self.clear_cache()\n",
    "\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                # blend the above tile and the left tile\n",
    "                # to the current tile and add the current tile to the result row\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_height)\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_width)\n",
    "                result_row.append(tile[:, :, :, :tile_latent_stride_height, :tile_latent_stride_width])\n",
    "            result_rows.append(torch.cat(result_row, dim=-1))\n",
    "\n",
    "        enc = torch.cat(result_rows, dim=3)[:, :, :, :latent_height, :latent_width]\n",
    "        return enc\n",
    "\n",
    "    def tiled_decode(self, z: torch.Tensor, return_dict: bool = True) -> Union[DecoderOutput, torch.Tensor]:\n",
    "        r\"\"\"\n",
    "        Decode a batch of images using a tiled decoder.\n",
    "\n",
    "        Args:\n",
    "            z (`torch.Tensor`): Input batch of latent vectors.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~models.vae.DecoderOutput`] instead of a plain tuple.\n",
    "\n",
    "        Returns:\n",
    "            [`~models.vae.DecoderOutput`] or `tuple`:\n",
    "                If return_dict is True, a [`~models.vae.DecoderOutput`] is returned, otherwise a plain `tuple` is\n",
    "                returned.\n",
    "        \"\"\"\n",
    "        _, _, num_frames, height, width = z.shape\n",
    "        sample_height = height * self.spatial_compression_ratio\n",
    "        sample_width = width * self.spatial_compression_ratio\n",
    "\n",
    "        tile_latent_min_height = self.tile_sample_min_height // self.spatial_compression_ratio\n",
    "        tile_latent_min_width = self.tile_sample_min_width // self.spatial_compression_ratio\n",
    "        tile_latent_stride_height = self.tile_sample_stride_height // self.spatial_compression_ratio\n",
    "        tile_latent_stride_width = self.tile_sample_stride_width // self.spatial_compression_ratio\n",
    "\n",
    "        blend_height = self.tile_sample_min_height - self.tile_sample_stride_height\n",
    "        blend_width = self.tile_sample_min_width - self.tile_sample_stride_width\n",
    "\n",
    "        # Split z into overlapping tiles and decode them separately.\n",
    "        # The tiles have an overlap to avoid seams between tiles.\n",
    "        rows = []\n",
    "        for i in range(0, height, tile_latent_stride_height):\n",
    "            row = []\n",
    "            for j in range(0, width, tile_latent_stride_width):\n",
    "                self.clear_cache()\n",
    "                time = []\n",
    "                for k in range(num_frames):\n",
    "                    self._conv_idx = [0]\n",
    "                    tile = z[:, :, k : k + 1, i : i + tile_latent_min_height, j : j + tile_latent_min_width]\n",
    "                    tile = self.post_quant_conv(tile)\n",
    "                    decoded = self.decoder(tile, feat_cache=self._feat_map, feat_idx=self._conv_idx)\n",
    "                    time.append(decoded)\n",
    "                row.append(torch.cat(time, dim=2))\n",
    "            rows.append(row)\n",
    "        self.clear_cache()\n",
    "\n",
    "        result_rows = []\n",
    "        for i, row in enumerate(rows):\n",
    "            result_row = []\n",
    "            for j, tile in enumerate(row):\n",
    "                # blend the above tile and the left tile\n",
    "                # to the current tile and add the current tile to the result row\n",
    "                if i > 0:\n",
    "                    tile = self.blend_v(rows[i - 1][j], tile, blend_height)\n",
    "                if j > 0:\n",
    "                    tile = self.blend_h(row[j - 1], tile, blend_width)\n",
    "                result_row.append(tile[:, :, :, : self.tile_sample_stride_height, : self.tile_sample_stride_width])\n",
    "            result_rows.append(torch.cat(result_row, dim=-1))\n",
    "\n",
    "        dec = torch.cat(result_rows, dim=3)[:, :, :, :sample_height, :sample_width]\n",
    "\n",
    "        if not return_dict:\n",
    "            return (dec,)\n",
    "        return DecoderOutput(sample=dec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多尺度压缩块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import einops\n",
    "\n",
    "class WanPatchEmbedForCleanLatents(nn.Module):\n",
    "    def __init__(self, inner_dim, in_chans=16):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1x压缩， 用于处理最近的关键帧（起始帧、结束帧等需要保留的关键帧）\n",
    "        self.proj = nn.Conv3d(in_chans, inner_dim, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        # 2x压缩， 用于处理中等时间尺度的上下文信息（最近生成的帧序列）\n",
    "        self.proj_2x = nn.Conv3d(in_chans, inner_dim, kernel_size=(2, 4, 4), stride=(2, 4, 4))\n",
    "        # 4x压缩， 用于处理长时间尺度的全局上下文信息（历史帧序列的全局信息）\n",
    "        self.proj_4x = nn.Conv3d(in_chans, inner_dim, kernel_size=(4, 8, 8), stride=(4, 8, 8))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def initialize_weight_from_another_conv3d(self, another_layer):\n",
    "        # 可能是训练或加载权重时使用\n",
    "        weight = another_layer.weight.detach().clone()\n",
    "        bias = another_layer.bias.detach().clone()\n",
    "\n",
    "        sd = {\n",
    "            # 1x压缩使用的权重\n",
    "            'proj.weight': weight.clone(),\n",
    "            'proj.bias': bias.clone(),\n",
    "            # 2x压缩使用的权重，将权重重复两倍，然后归一化（有三个重复了两倍，所以要除以8）\n",
    "            'proj_2x.weight': einops.repeat(weight, 'b c t h w -> b c (t tk) (h hk) (w wk)', tk=2, hk=2, wk=2) / 8.0,\n",
    "            'proj_2x.bias': bias.clone(),\n",
    "            # 4x压缩使用的权重，将权重重复四倍，然后归一化（有三个重复了四倍，所以要除以64）\n",
    "            'proj_4x.weight': einops.repeat(weight, 'b c t h w -> b c (t tk) (h hk) (w wk)', tk=4, hk=4, wk=4) / 64.0,\n",
    "            'proj_4x.bias': bias.clone(),\n",
    "        }\n",
    "\n",
    "        sd = {k: v.clone() for k, v in sd.items()}\n",
    "\n",
    "        self.load_state_dict(sd)\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为什么使用压缩后需要重复对应的参数？\n",
    "\n",
    "如果直接用随机权重初始化，不同压缩比例会产生完全不同的特征表示，破坏语义一致性\n",
    "推理代码中应该没有直接使用，可能在训练时使用\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from diffusers.models.transformers.transformer_wan import WanTransformer3DModel\n",
    "from diffusers.models.transformers.transformer_wan import WanRotaryPosEmbed\n",
    "from diffusers.models.transformers.transformer_wan import WanTimeTextImageEmbedding\n",
    "from torch import nn\n",
    "from typing import Dict, Tuple, Optional, Union\n",
    "\n",
    "class WanTransformer3DModelPacked(WanTransformer3DModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_size: Tuple[int] = (1, 2, 2),\n",
    "        num_attention_heads: int = 40,\n",
    "        attention_head_dim: int = 128,\n",
    "        in_channels: int = 16,\n",
    "        out_channels: int = 16,\n",
    "        text_dim: int = 4096,\n",
    "        freq_dim: int = 256,\n",
    "        ffn_dim: int = 13824,\n",
    "        num_layers: int = 40,\n",
    "        cross_attn_norm: bool = True,\n",
    "        qk_norm: Optional[str] = \"rms_norm_across_heads\",\n",
    "        eps: float = 1e-6,\n",
    "        image_dim: Optional[int] = None,\n",
    "        added_kv_proj_dim: Optional[int] = None,\n",
    "        rope_max_seq_len: int = 1024,\n",
    "        \n",
    "        has_clean_embedding: bool=False\n",
    "    ) -> None:\n",
    "        super().__init__(patch_size, num_attention_heads, attention_head_dim, in_channels, out_channels, text_dim, freq_dim, ffn_dim, num_layers, cross_attn_norm, qk_norm, eps, image_dim, added_kv_proj_dim, rope_max_seq_len)\n",
    "        \n",
    "        \n",
    "        self.inner_dim = num_attention_heads * attention_head_dim\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # 设置多尺度压缩层\n",
    "        self.clean_embedding = None\n",
    "        if has_clean_embedding:\n",
    "            self.install_clean_embedding()\n",
    "            \n",
    "        # print(f\"WanTransformer3DModelPacked initialized with {self.inner_dim} inner dimensions\")\n",
    "            \n",
    "        # 1. Patch & position embedding\n",
    "        \n",
    "        # 2. Condition embeddings\n",
    "        \n",
    "        # 3. Transformer blocks\n",
    "        \n",
    "        # 4. Output norm & projection\n",
    "    \n",
    "\n",
    "    def install_clean_embedding(self):\n",
    "        \"\"\"\n",
    "        应用多尺度压缩层，将输入的latents压缩到text_dim维度\n",
    "        \"\"\"\n",
    "        self.clean_embedding = WanPatchEmbedForCleanLatents(self.inner_dim, self.in_channels)\n",
    "        self.config['has_clean_embedding'] = True\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor, timestep: torch.LongTensor, encoder_hidden_states: torch.Tensor, encoder_hidden_states_image: Optional[torch.Tensor] = None,\n",
    "                latent_indices=None,\n",
    "            clean_latents=None, clean_latent_indices=None,\n",
    "            clean_latents_2x=None, clean_latent_2x_indices=None,\n",
    "            clean_latents_4x=None, clean_latent_4x_indices=None,\n",
    "            return_dict=True, attention_kwargs=None)->Union[torch.Tensor, Dict[str, torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        TODO: 使用多尺度压缩层，将输入的latents压缩\n",
    "        \"\"\"\n",
    "        # print(f\"hidden_states.shape = {hidden_states.shape}, device = {hidden_states.device}, dtype = {hidden_states.dtype}\" if hidden_states is not None else \"hidden_states is None\")\n",
    "        # print(f\"timestep = {timestep}, device = {timestep.device}, dtype = {timestep.dtype}\" if timestep is not None else \"timestep is None\")\n",
    "        # print(f\"encoder_hidden_states.shape = {encoder_hidden_states.shape}, device = {encoder_hidden_states.device}, dtype = {encoder_hidden_states.dtype}\" if encoder_hidden_states is not None else \"encoder_hidden_states is None\")\n",
    "        # print(f\"latent_indices.shape = {latent_indices.shape}, device = {latent_indices.device}, dtype = {latent_indices.dtype}\" if latent_indices is not None else \"latent_indices is None\")\n",
    "        # print(f\"clean_latents.shape = {clean_latents.shape}, device = {clean_latents.device}, dtype = {clean_latents.dtype}\" if clean_latents is not None else \"clean_latents is None\")\n",
    "        # print(f\"clean_latent_indices.shape = {clean_latent_indices.shape}, device = {clean_latent_indices.device}, dtype = {clean_latent_indices.dtype}\" if clean_latent_indices is not None else \"clean_latent_indices is None\")\n",
    "        # print(f\"clean_latents_2x.shape = {clean_latents_2x.shape}, device = {clean_latents_2x.device}, dtype = {clean_latents_2x.dtype}\" if clean_latents_2x is not None else \"clean_latents_2x is None\")\n",
    "        # print(f\"clean_latent_2x_indices.shape = {clean_latent_2x_indices.shape}, device = {clean_latent_2x_indices.device}, dtype = {clean_latent_2x_indices.dtype}\" if clean_latent_2x_indices is not None else \"clean_latent_2x_indices is None\")\n",
    "        # print(f\"clean_latents_4x.shape = {clean_latents_4x.shape}, device = {clean_latents_4x.device}, dtype = {clean_latents_4x.dtype}\" if clean_latents_4x is not None else \"clean_latents_4x is None\")\n",
    "        # print(f\"clean_latent_4x_indices.shape = {clean_latent_4x_indices.shape}, device = {clean_latent_4x_indices.device}, dtype = {clean_latent_4x_indices.dtype}\" if clean_latent_4x_indices is not None else \"clean_latent_4x_indices is None\")\n",
    "        # print(f\"transformer device: {next(self.parameters()).device}\")\n",
    "        # print(f\"transformer dtype: {next(self.parameters()).dtype}\")\n",
    "        \n",
    "        return super().forward(hidden_states, timestep, encoder_hidden_states, encoder_hidden_states_image, return_dict, attention_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers.utils import is_torch_xla_available\n",
    "if is_torch_xla_available():\n",
    "    import torch_xla.core.xla_model as xm\n",
    "\n",
    "    XLA_AVAILABLE = True\n",
    "else:\n",
    "    XLA_AVAILABLE = False\n",
    "\n",
    "XLA_AVAILABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的WanFramePackPipeline实现\n",
    "from typing import Any, Callable, List, Union, Optional, Dict\n",
    "from diffusers.callbacks import MultiPipelineCallbacks, PipelineCallback\n",
    "from diffusers.pipelines.wan.pipeline_output import WanPipelineOutput\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class WanFramePackPipelineComplete(WanPipeline):\n",
    "    \"\"\"\n",
    "    完整支持FramePack功能的Wan Pipeline\n",
    "    \"\"\"\n",
    "    def __init__(self, tokenizer, text_encoder, transformer, vae, scheduler, high_vram=False, latent_window_size=9, cfg=1.0, rs=0.0, target_device=None):\n",
    "        self.high_vram = high_vram\n",
    "        self.latent_window_size = latent_window_size\n",
    "        self.cfg = cfg\n",
    "        self.rs = rs\n",
    "        self.total_latent_sections = None\n",
    "        self.prompt_embeds = None\n",
    "        self.negative_prompt_embeds = None\n",
    "        \n",
    "        self.target_device = target_device\n",
    "        if self.target_device is None:\n",
    "            self.target_device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "        super().__init__(tokenizer, text_encoder, transformer, vae, scheduler)\n",
    "        \n",
    "        if not self.high_vram:\n",
    "            from diffusers_helper.memory import DynamicSwapInstaller\n",
    "            DynamicSwapInstaller.install_model(self.transformer, device=self._execution_device)\n",
    "            DynamicSwapInstaller.install_model(self.text_encoder, device=self._execution_device)\n",
    "        \n",
    "    def encode_prompt(self, \n",
    "                      prompt: Union[str, List[str]],\n",
    "                      negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "                      do_classifier_free_guidance: bool = True,\n",
    "                      num_videos_per_prompt: int = 1,\n",
    "                      prompt_embeds: Optional[torch.Tensor] = None,\n",
    "                      negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "                      max_sequence_length: int = 226,\n",
    "                      device: Optional[torch.device] = None,\n",
    "                      dtype: Optional[torch.dtype] = None):\n",
    "        \"\"\"FramePack专用的文本编码方法，会缓存结果\"\"\"\n",
    "        if not self.high_vram:\n",
    "            unload_complete_models()\n",
    "            fake_diffusers_current_device(self.text_encoder, device)\n",
    "            load_model_as_complete(self.text_encoder, target_device=device)\n",
    "\n",
    "        prompt_embeds, negative_prompt_embeds = super().encode_prompt(\n",
    "            prompt, negative_prompt, do_classifier_free_guidance, \n",
    "            num_videos_per_prompt, prompt_embeds, negative_prompt_embeds, \n",
    "            max_sequence_length, device=device, dtype=dtype)\n",
    "        \n",
    "        # 缓存编码结果\n",
    "        self.prompt_embeds = prompt_embeds\n",
    "        self.negative_prompt_embeds = negative_prompt_embeds\n",
    "        \n",
    "        # if not self.high_vram:\n",
    "        #     unload_complete_models()\n",
    "        \n",
    "        return prompt_embeds, negative_prompt_embeds\n",
    "    \n",
    "    def sample_framepack_section(self, \n",
    "                                latents: torch.Tensor,\n",
    "                                clean_latents: torch.Tensor,\n",
    "                                clean_latent_indices: torch.Tensor,\n",
    "                                clean_latents_2x: torch.Tensor,\n",
    "                                clean_latent_2x_indices: torch.Tensor,\n",
    "                                clean_latents_4x: torch.Tensor,\n",
    "                                clean_latent_4x_indices: torch.Tensor,\n",
    "                                prompt_embeds: torch.Tensor,\n",
    "                                negative_prompt_embeds: Optional[torch.Tensor],\n",
    "                                num_inference_steps: int,\n",
    "                                guidance_scale: float,\n",
    "                                generator: Optional[torch.Generator] = None,\n",
    "                                attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                                callback_on_step_end: Optional[Callable] = None,\n",
    "                                callback_on_step_end_tensor_inputs: List[str] = [\"latents\"]):\n",
    "        \"\"\"\n",
    "        FramePack单段采样函数\n",
    "        \"\"\"\n",
    "        device = self._execution_device\n",
    "        \n",
    "        # 设置时间步\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "        \n",
    "        # 确保latents在正确设备和数据类型\n",
    "        latents = latents.to(device=device, dtype=self.transformer.dtype)\n",
    "        \n",
    "        # 准备FramePack参数\n",
    "        framepack_kwargs = {\n",
    "            'clean_latents': clean_latents.to(device=device, dtype=self.transformer.dtype),\n",
    "            'clean_latent_indices': clean_latent_indices.to(device=device),\n",
    "            'clean_latents_2x': clean_latents_2x.to(device=device, dtype=self.transformer.dtype),\n",
    "            'clean_latent_2x_indices': clean_latent_2x_indices.to(device=device),\n",
    "            'clean_latents_4x': clean_latents_4x.to(device=device, dtype=self.transformer.dtype),\n",
    "            'clean_latent_4x_indices': clean_latent_4x_indices.to(device=device),\n",
    "        }\n",
    "        \n",
    "        # 去噪循环\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        transformer_dtype = self.transformer.dtype\n",
    "        # with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "        for i, t in enumerate(timesteps):\n",
    "            if self.interrupt:\n",
    "                continue\n",
    "                \n",
    "            self._current_timestep = t\n",
    "            latent_model_input = latents.to(transformer_dtype).to(device)\n",
    "            timestep = t.expand(latents.shape[0])\n",
    "            \n",
    "            if not self.high_vram:\n",
    "                unload_complete_models()\n",
    "                # fake_diffusers_current_device(self.transformer, device)\n",
    "                # load_model_as_complete(self.transformer, target_device=device)\n",
    "                move_model_to_device_with_memory_preservation(self.transformer, target_device=device, preserved_memory_gb=self.gpu_memory_preservation)\n",
    "            \n",
    "            # 正向预测（带FramePack参数）\n",
    "            noise_pred = self.transformer(\n",
    "                hidden_states=latent_model_input,\n",
    "                timestep=timestep,\n",
    "                encoder_hidden_states=prompt_embeds.to(device),\n",
    "                attention_kwargs=attention_kwargs,\n",
    "                return_dict=False,\n",
    "                **framepack_kwargs  # 传递FramePack参数\n",
    "            )[0]\n",
    "            # print(f\"[WanFramePackPipelineComplete.sample_framepack_section] noise_pred.shape = {noise_pred.shape}\")\n",
    "            # 分类器自由引导\n",
    "            if self.do_classifier_free_guidance:\n",
    "                noise_uncond = self.transformer(\n",
    "                    hidden_states=latent_model_input,\n",
    "                    timestep=timestep,\n",
    "                    encoder_hidden_states=negative_prompt_embeds.to(device) if negative_prompt_embeds is not None else None,\n",
    "                    attention_kwargs=attention_kwargs,\n",
    "                    return_dict=False,\n",
    "                    **framepack_kwargs  # 传递FramePack参数\n",
    "                )[0]\n",
    "                noise_pred = noise_uncond + guidance_scale * (noise_pred - noise_uncond)\n",
    "            \n",
    "            # 计算前一个噪声样本 x_t -> x_t-1\n",
    "            latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "            \n",
    "            # 回调处理\n",
    "            if callback_on_step_end is not None:\n",
    "                callback_kwargs = {}\n",
    "                for k in callback_on_step_end_tensor_inputs:\n",
    "                    callback_kwargs[k] = locals()[k]\n",
    "                callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "                \n",
    "                latents = callback_outputs.pop(\"latents\", latents)\n",
    "                prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "                negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n",
    "            \n",
    "            # if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "            #     progress_bar.update()\n",
    "                \n",
    "            if XLA_AVAILABLE:\n",
    "                xm.mark_step()\n",
    "        \n",
    "        self._current_timestep = None\n",
    "        return latents\n",
    "    \n",
    "    @property\n",
    "    def _execution_device(self):\n",
    "        \"\"\"\n",
    "        FramePack 重写 _execution_device 属性，确保返回正确的 GPU 设备(暂时设置为cuda:0)\n",
    "        \"\"\"\n",
    "        # 如果使用高显存模式，直接返回 GPU\n",
    "        if self.high_vram:\n",
    "            return self.target_device\n",
    "        \n",
    "        # 对于低显存模式，检查是否有模型在 GPU 上\n",
    "        for name, component in self.components.items():\n",
    "            if isinstance(component, torch.nn.Module) and hasattr(component, 'device'):\n",
    "                if component.device.type == 'cuda':\n",
    "                    return component.device\n",
    "        \n",
    "        # 如果没有模型在 GPU 上，返回默认的 GPU 设备\n",
    "        # 这是 FramePack 的关键：即使模型在 CPU，执行设备仍然是 GPU\n",
    "        return self.target_device\n",
    "    \n",
    "    def prepare_history_latents(self, \n",
    "                                batch_size: int,\n",
    "                                num_channels_latents: int,\n",
    "                                height: int,\n",
    "                                width: int,\n",
    "                                dtype: torch.dtype,\n",
    "                                device: torch.device,\n",
    "                                num_frames: int = None,\n",
    "                                num_latent_frames: int = None\n",
    "    ):\n",
    "        # 优先使用num_latent_frames, 如果num_latent_frames为None, 则根据num_frames计算\n",
    "        assert num_frames is not None or num_latent_frames is not None, \"num_frames or num_latent_frames must be provided\"\n",
    "        if not num_latent_frames:\n",
    "            num_latent_frames = (num_frames - 1) // self.vae_scale_factor_temporal + 1\n",
    "        \n",
    "        shape = (\n",
    "            batch_size, num_channels_latents, num_latent_frames, int(height) // self.vae_scale_factor_spatial, int(width) // self.vae_scale_factor_spatial\n",
    "        )            \n",
    "        history_latents = torch.zeros(\n",
    "            size=shape,\n",
    "            dtype=dtype,\n",
    "            device=device\n",
    "        )\n",
    "        return history_latents\n",
    "    \n",
    "    def __call__(self, \n",
    "                prompt: Union[str, List[str]] = None,\n",
    "                negative_prompt: Union[str, List[str]] = None,\n",
    "                height: int = 480,\n",
    "                width: int = 832,\n",
    "                num_inference_steps: int = 50,\n",
    "                guidance_scale: float = 5.0,\n",
    "                num_videos_per_prompt: Optional[int] = 1,\n",
    "                generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "                latents: Optional[torch.Tensor] = None,\n",
    "                prompt_embeds: Optional[torch.Tensor] = None,\n",
    "                negative_prompt_embeds: Optional[torch.Tensor] = None,\n",
    "                output_type: Optional[str] = \"np\",\n",
    "                return_dict: bool = True,\n",
    "                attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "                callback_on_step_end: Optional[\n",
    "                    Union[Callable[[int, int, Dict], None], PipelineCallback, MultiPipelineCallbacks]\n",
    "                ] = None,\n",
    "                callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "                max_sequence_length: int = 512,\n",
    "                seed: int = 42,\n",
    "                use_teacache: bool = False,\n",
    "                total_second_length: float = 5.0,\n",
    "                fps: float = 30.0,\n",
    "                gpu_memory_preservation: float = 11.0):\n",
    "        \"\"\"\n",
    "        FramePack视频生成主函数\n",
    "        \"\"\"\n",
    "        device = self._execution_device\n",
    "        # print(f\"[WanFramePackPipelineComplete.__call__] device = {device}\")\n",
    "        \n",
    "        # 1. 输入检查\n",
    "        self.check_inputs(\n",
    "            prompt, negative_prompt, height, width,\n",
    "            prompt_embeds, negative_prompt_embeds, callback_on_step_end_tensor_inputs\n",
    "        )\n",
    "        \n",
    "        height, width = find_nearest_bucket(height, width, resolution=640)\n",
    "        \n",
    "        # 2. 设置参数\n",
    "        self._guidance_scale = guidance_scale\n",
    "        self._attention_kwargs = attention_kwargs\n",
    "        self._current_timestep = None\n",
    "        self._interrupt = False\n",
    "        self.gpu_memory_preservation = gpu_memory_preservation\n",
    "        \n",
    "        if isinstance(callback_on_step_end, (PipelineCallback, MultiPipelineCallbacks)):\n",
    "            callback_on_step_end_tensor_inputs = callback_on_step_end.tensor_inputs\n",
    "        \n",
    "        # 3. 文本编码\n",
    "        transformer_dtype = self.transformer.dtype\n",
    "        if self.prompt_embeds is None or self.negative_prompt_embeds is None:\n",
    "            prompt_embeds, negative_prompt_embeds = self.encode_prompt(\n",
    "                prompt=prompt,\n",
    "                negative_prompt=negative_prompt,\n",
    "                do_classifier_free_guidance=self.do_classifier_free_guidance,\n",
    "                num_videos_per_prompt=num_videos_per_prompt,\n",
    "                prompt_embeds=prompt_embeds,\n",
    "                negative_prompt_embeds=negative_prompt_embeds,\n",
    "                max_sequence_length=max_sequence_length,\n",
    "                device=device,\n",
    "                dtype=transformer_dtype\n",
    "            )\n",
    "        else:\n",
    "            prompt_embeds = self.prompt_embeds\n",
    "            negative_prompt_embeds = self.negative_prompt_embeds\n",
    "            \n",
    "            prompt_embeds = prompt_embeds.to(transformer_dtype)\n",
    "            if negative_prompt_embeds is not None:\n",
    "                negative_prompt_embeds = negative_prompt_embeds.to(transformer_dtype)\n",
    "        # print(f\"[WanFramePackPipelineComplete.__call__] prompt_embeds.shape = {self.prompt_embeds.shape}, negative_prompt_embeds.shape = {self.negative_prompt_embeds.shape}\")\n",
    "        \n",
    "        \n",
    "        # 4. 准备时间步\n",
    "        # print(f\"[WanFramePackPipelineComplete.__call__] num_inference_steps = {num_inference_steps}\")\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "        \n",
    "        \n",
    "        # 5. 创建起始latent\n",
    "        # print(f\"[WanFramePackPipelineComplete.__call__] start prepare start latent variables\")\n",
    "        num_channels_latents = self.transformer.config.in_channels\n",
    "        if generator is None:\n",
    "            generator = torch.Generator(device=cpu).manual_seed(seed)\n",
    "\n",
    "        # 生成起始latent\n",
    "        # prepare_latents 会压缩width和height\n",
    "        # 如果start_latent存在，prepare_latents会直接返回start_latent\n",
    "        if not self.high_vram:\n",
    "            # print(f\"[WanFramePackPipelineComplete.__call__] load vae model\")\n",
    "            load_model_as_complete(self.vae, target_device=device)\n",
    "            # print(f\"[WanFramePackPipelineComplete.__call__] load vae model done\")\n",
    "\n",
    "        # print(f\"[WanFramePackPipelineComplete.__call__] start prepare start latent variables\")\n",
    "        start_latent = self.prepare_latents(\n",
    "            batch_size=1,\n",
    "            num_channels_latents=num_channels_latents,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            num_frames=1,\n",
    "            dtype=self.vae.dtype,   # 官方直接使用torch.float32\n",
    "            device=cpu,\n",
    "            generator=generator,\n",
    "            latents=latents\n",
    "        )\n",
    "        # print(f\"[WanFramePackPipelineComplete.__call__] start_latent.shape = {start_latent.shape}\")\n",
    "        \n",
    "        # 6. FramePack核心逻辑, 默认窗口大小为9, num_frames = 33, 每个section的latent数量为33 * 16 = 528\n",
    "        # 这里的4应该是指时间压缩倍率，3是指下面history_latents压缩后占用的latent数\n",
    "        num_frames = self.latent_window_size * 4 - 3\n",
    "        total_latent_sections = int(max(round((total_second_length * fps) / (self.latent_window_size * 4)), 1))\n",
    "        # print(f\"[WanFramePackPipelineComplete.__call__] total_latent_sections = {total_latent_sections}\")\n",
    "        \n",
    "        # 初始化历史latents缓冲区\n",
    "        # 1，2，16分别指1x, 2x, 4x压缩（前）的latents，后面经过不同卷积核后变成3个latent\n",
    "        history_latents = self.prepare_history_latents(\n",
    "            batch_size=1,\n",
    "            num_channels_latents=num_channels_latents,\n",
    "            num_latent_frames= 1 + 2 + 16,\n",
    "            height=height,\n",
    "            width=width,\n",
    "            dtype=self.vae.dtype,\n",
    "            device=cpu\n",
    "        )\n",
    "        \n",
    "        history_pixels = None\n",
    "        total_generated_latent_frames = 0\n",
    "        \n",
    "        # 设置latent_paddings序列\n",
    "        if total_latent_sections <= 4:\n",
    "            # 理论上，`latent_paddings` 应该遵循上述顺序，但当 `total_latent_sections` 大于4时，复制某些元素似乎比扩展它的效果更好。\n",
    "            # 可以尝试去掉下面这个技巧，只使用 `latent_paddings = list(reversed(range(total_latent_sections)))` 来进行比较。 \n",
    "            latent_paddings = list(reversed(range(total_latent_sections)))\n",
    "        else:\n",
    "            latent_paddings = [3] + [2] * (total_latent_sections - 3) + [1, 0]\n",
    "        \n",
    "        # 7. 分段生成循环\n",
    "        for section_idx, latent_padding in enumerate(latent_paddings):\n",
    "            is_last_section = latent_padding == 0\n",
    "            latent_padding_size = latent_padding * self.latent_window_size\n",
    "            \n",
    "            print(f'Section {section_idx+1}/{len(latent_paddings)}: '\n",
    "                  f'latent_padding_size={latent_padding_size}, is_last_section={is_last_section}')\n",
    "            \n",
    "            # 构建FramePack的多尺度索引\n",
    "            total_indices = sum([1, latent_padding_size, self.latent_window_size, 1, 2, 16])\n",
    "            indices = torch.arange(0, total_indices).unsqueeze(0)\n",
    "            \n",
    "            split_sizes = [1, latent_padding_size, self.latent_window_size, 1, 2, 16]\n",
    "            (clean_latent_indices_pre, blank_indices, latent_indices, \n",
    "             clean_latent_indices_post, clean_latent_2x_indices, clean_latent_4x_indices) = indices.split(split_sizes, dim=1)\n",
    "            \n",
    "            clean_latent_indices = torch.cat([clean_latent_indices_pre, clean_latent_indices_post], dim=1)\n",
    "            \n",
    "            # 构建多尺度clean_latents\n",
    "            clean_latents_pre = start_latent.to(history_latents.device, history_latents.dtype)\n",
    "            clean_latents_post, clean_latents_2x, clean_latents_4x = history_latents[:, :, :1 + 2 + 16, :, :].split([1, 2, 16], dim=2)\n",
    "            clean_latents = torch.cat([clean_latents_pre, clean_latents_post], dim=2)\n",
    "            \n",
    "            # 准备当前段的噪声latents\n",
    "            current_latents = self.prepare_latents(\n",
    "                batch_size=1,\n",
    "                num_channels_latents=num_channels_latents,\n",
    "                height=height,\n",
    "                width=width,\n",
    "                num_frames=num_frames,\n",
    "                dtype=self.vae.dtype,\n",
    "                device=cpu,\n",
    "                generator=generator,\n",
    "                latents=None\n",
    "            )\n",
    "            \n",
    "            # 模型加载到GPU\n",
    "            if not self.high_vram:\n",
    "                unload_complete_models()\n",
    "                move_model_to_device_with_memory_preservation(\n",
    "                    self.transformer, target_device=device, \n",
    "                    preserved_memory_gb=gpu_memory_preservation\n",
    "                )\n",
    "            \n",
    "            # TeaCache支持（如果需要）\n",
    "            if use_teacache:\n",
    "                print(\"TeaCache not implemented yet\")\n",
    "            \n",
    "            # 执行FramePack采样\n",
    "            generated_latents = self.sample_framepack_section(\n",
    "                latents=current_latents,\n",
    "                clean_latents=clean_latents,\n",
    "                clean_latent_indices=clean_latent_indices,\n",
    "                clean_latents_2x=clean_latents_2x,\n",
    "                clean_latent_2x_indices=clean_latent_2x_indices,\n",
    "                clean_latents_4x=clean_latents_4x,\n",
    "                clean_latent_4x_indices=clean_latent_4x_indices,\n",
    "                prompt_embeds=prompt_embeds,\n",
    "                negative_prompt_embeds=negative_prompt_embeds,\n",
    "                num_inference_steps=num_inference_steps,\n",
    "                guidance_scale=guidance_scale,\n",
    "                generator=generator,\n",
    "                attention_kwargs=attention_kwargs,\n",
    "                callback_on_step_end=callback_on_step_end,\n",
    "                callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs\n",
    "            )\n",
    "            \n",
    "            # 处理最后一段\n",
    "            if is_last_section:\n",
    "                generated_latents = torch.cat([start_latent.to(generated_latents), generated_latents], dim=2)\n",
    "            \n",
    "            # 更新历史缓冲区\n",
    "            total_generated_latent_frames += int(generated_latents.shape[2])\n",
    "            history_latents = torch.cat([generated_latents.to(history_latents), history_latents], dim=2)\n",
    "            \n",
    "            # 内存管理\n",
    "            if not self.high_vram:\n",
    "                offload_model_from_device_for_memory_preservation(\n",
    "                    self.transformer, target_device=device, preserved_memory_gb=self.gpu_memory_preservation\n",
    "                )\n",
    "                load_model_as_complete(self.vae, target_device=device)\n",
    "            \n",
    "            # VAE解码（如果需要输出视频）\n",
    "            if output_type != \"latent\":\n",
    "                real_history_latents = history_latents[:, :, :total_generated_latent_frames, :, :]\n",
    "                \n",
    "                if history_pixels is None:\n",
    "                    # 首次解码\n",
    "                    history_pixels = self.vae.decode(\n",
    "                        real_history_latents.to(self.vae.dtype).to(self.vae.device)\n",
    "                    ).sample.cpu()\n",
    "                else:\n",
    "                    # 增量解码\n",
    "                    section_latent_frames = (self.latent_window_size * 2 + 1) if is_last_section else (self.latent_window_size * 2)\n",
    "                    overlapped_frames = self.latent_window_size * 4 - 3\n",
    "                    \n",
    "                    current_pixels = self.vae.decode(\n",
    "                        real_history_latents[:, :, :section_latent_frames].to(self.vae.dtype).to(self.vae.device)\n",
    "                    ).sample.cpu()\n",
    "                    \n",
    "                    history_pixels = soft_append_bcthw(current_pixels, history_pixels, overlapped_frames)\n",
    "            \n",
    "            # 内存清理\n",
    "            if not self.high_vram:\n",
    "                unload_complete_models()\n",
    "            \n",
    "            print(f'Section {section_idx+1} completed. Generated frames: {total_generated_latent_frames}')\n",
    "            \n",
    "            if is_last_section:\n",
    "                break\n",
    "        \n",
    "        # 8. 最终输出处理\n",
    "        if output_type == \"latent\":\n",
    "            video = history_latents[:, :, :total_generated_latent_frames, :, :]\n",
    "        else:\n",
    "            video = self.video_processor.postprocess_video(history_pixels, output_type=output_type)\n",
    "        \n",
    "        # 清理\n",
    "        self.maybe_free_model_hooks()\n",
    "        \n",
    "        if not return_dict:\n",
    "            return (video,)\n",
    "        \n",
    "        return WanPipelineOutput(frames=video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 2 files: 100%|██████████| 2/2 [00:00<00:00, 11169.92it/s]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.18s/it]\n"
     ]
    }
   ],
   "source": [
    "transformer = WanTransformer3DModelPacked.from_pretrained(model_id, subfolder=\"transformer\", torch_dtype=torch.bfloat16, has_clean_x_embedder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free VRAM 10.7939453125 GB\n",
      "High-VRAM Mode: False\n",
      "Loading Wan2.1-1.3B model components...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 5/5 [00:00<00:00, 14065.41it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading checkpoint shards: 100%|██████████| 5/5 [01:22<00:00, 16.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using WanTransformer3DModelPacked with FramePack features...\n",
      "Enabling tiled VAE decoding. This will split the input tensor into tiles to compute decoding in several steps.\n",
      "transformer.high_quality_fp32_output_for_inference = True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WanTransformer3DModelPacked(\n",
       "  (rope): WanRotaryPosEmbed()\n",
       "  (patch_embedding): Conv3d(16, 1536, kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
       "  (condition_embedder): WanTimeTextImageEmbedding(\n",
       "    (timesteps_proj): Timesteps()\n",
       "    (time_embedder): TimestepEmbedding(\n",
       "      (linear_1): Linear(in_features=256, out_features=1536, bias=True)\n",
       "      (act): SiLU()\n",
       "      (linear_2): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    )\n",
       "    (act_fn): SiLU()\n",
       "    (time_proj): Linear(in_features=1536, out_features=9216, bias=True)\n",
       "    (text_embedder): PixArtAlphaTextProjection(\n",
       "      (linear_1): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "      (act_1): GELU(approximate='tanh')\n",
       "      (linear_2): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-29): 30 x WanTransformerBlock(\n",
       "      (norm1): FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn1): Attention(\n",
       "        (norm_q): RMSNorm()\n",
       "        (norm_k): RMSNorm()\n",
       "        (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (attn2): Attention(\n",
       "        (norm_q): RMSNorm()\n",
       "        (norm_k): RMSNorm()\n",
       "        (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (ffn): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GELU(\n",
       "            (proj): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm3): FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "    )\n",
       "  )\n",
       "  (norm_out): FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "  (proj_out): Linear(in_features=1536, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取显存信息\n",
    "free_mem_gb = get_cuda_free_memory_gb(gpu)\n",
    "high_vram = free_mem_gb > 60\n",
    "\n",
    "print(f'Free VRAM {free_mem_gb} GB')\n",
    "print(f'High-VRAM Mode: {high_vram}')\n",
    "\n",
    "# 加载模型组件\n",
    "print(\"Loading Wan2.1-1.3B model components...\")\n",
    "\n",
    "# 加载tokenizer和text_encoder\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, subfolder='tokenizer')\n",
    "text_encoder = UMT5EncoderModel.from_pretrained(model_id, subfolder='text_encoder', torch_dtype=torch.bfloat16).cpu()\n",
    "\n",
    "# 加载VAE\n",
    "vae = AutoencoderKLWan.from_pretrained(model_id, subfolder=\"vae\", torch_dtype=torch.float16).cpu()\n",
    "\n",
    "# 加载调度器\n",
    "scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\n",
    "\n",
    "# 使用我们的WanTransformer3DModelPacked\n",
    "print(\"Using WanTransformer3DModelPacked with FramePack features...\")\n",
    "\n",
    "# 设置模型为评估模式\n",
    "vae.eval()\n",
    "text_encoder.eval()\n",
    "transformer.eval()\n",
    "\n",
    "if not high_vram:\n",
    "    vae.enable_slicing()\n",
    "    vae.enable_tiling()\n",
    "\n",
    "# 设置高精度输出\n",
    "transformer.high_quality_fp32_output_for_inference = True\n",
    "print('transformer.high_quality_fp32_output_for_inference = True')\n",
    "\n",
    "# 设置数据类型\n",
    "transformer.to(dtype=torch.bfloat16)\n",
    "vae.to(dtype=torch.float16)\n",
    "text_encoder.to(dtype=torch.bfloat16)\n",
    "\n",
    "# 冻结梯度\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "transformer.requires_grad_(False)\n",
    "\n",
    "# 内存管理\n",
    "# if not high_vram:\n",
    "#     from diffusers_helper.memory import DynamicSwapInstaller\n",
    "#     DynamicSwapInstaller.install_model(transformer, device=gpu)\n",
    "#     DynamicSwapInstaller.install_model(text_encoder, device=gpu)\n",
    "# else:\n",
    "#     text_encoder.to(gpu)\n",
    "#     vae.to(gpu)\n",
    "#     transformer.to(gpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = WanFramePackPipelineComplete(\n",
    "    tokenizer=tokenizer,\n",
    "    text_encoder=text_encoder, \n",
    "    transformer=transformer,\n",
    "    vae=vae,\n",
    "    scheduler=scheduler,\n",
    "    high_vram=high_vram,\n",
    "    target_device=gpu\n",
    ")\n",
    "\n",
    "# 暂时使用标准pipeline进行测试\n",
    "# pipeline = WanPipeline.from_pretrained(\n",
    "#     model_id,\n",
    "#     transformer=transformer,  # 使用我们的packed transformer\n",
    "#     torch_dtype=torch.bfloat16\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section 1/2: latent_padding_size=9, is_last_section=False\n",
      "Section 1 completed. Generated frames: 9\n",
      "Section 2/2: latent_padding_size=0, is_last_section=True\n",
      "Section 2 completed. Generated frames: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(73, 480, 832, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers.utils import export_to_video\n",
    "\n",
    "prompt = \"A cat walks on the grass, realistic\"\n",
    "negative_prompt = \"Bright tones, overexposed, static, blurred details, subtitles, style, works, paintings, images, static, overall gray, worst quality, low quality, JPEG compression residue, ugly, incomplete, extra fingers, poorly drawn hands, poorly drawn faces, deformed, disfigured, misshapen limbs, fused fingers, still picture, messy background, three legs, many people in the background, walking backwards\"\n",
    "output = pipeline(\n",
    "            prompt=prompt,\n",
    "            negative_prompt=negative_prompt,\n",
    "            height=480,\n",
    "            width=832,\n",
    "            guidance_scale=5.0,\n",
    "            total_second_length=2,\n",
    "            gpu_memory_preservation=11.0,\n",
    "            fps=30,\n",
    "        ).frames[0]\n",
    "\n",
    "# 设置输出目录\n",
    "outputs_folder = \"/home/tippy/FramePack/outputs\"\n",
    "os.makedirs(outputs_folder, exist_ok=True)\n",
    "output_path =  os.path.join(outputs_folder, \"test_output.mp4\")\n",
    "export_to_video(output, output_path, fps=30)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"400\"\n",
       "            height=\"300\"\n",
       "            src=\"https://player.vimeo.com/video//home/tippy/FramePack/outputs/test_output.mp4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.VimeoVideo at 0x7fa7b41bc490>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_latent = torch.randn(\n",
    "                (1, 16, 1, 1080 // 8, 1920 // 8),\n",
    "                generator=torch.Generator(gpu).manual_seed(42),\n",
    "                dtype=vae.dtype,\n",
    "                device=gpu\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 19, 480, 480])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_latents = torch.zeros(1, 16, 1+2+16, 480, 480)\n",
    "history_latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 16, 480, 480])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_latents = history_latents.split([1, 2, 16], dim=2)\n",
    "history_latents[0].shape\n",
    "history_latents[1].shape\n",
    "history_latents[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_internal_dict', 'training', '_parameters', '_buffers', '_non_persistent_buffers_set', '_backward_pre_hooks', '_backward_hooks', '_is_full_backward_hook', '_forward_hooks', '_forward_hooks_with_kwargs', '_forward_hooks_always_called', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_state_dict_hooks', '_state_dict_pre_hooks', '_load_state_dict_pre_hooks', '_load_state_dict_post_hooks', '_modules', '_gradient_checkpointing_func', 'gradient_checkpointing', 'inner_dim', 'in_channels'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_internal_dict': FrozenDict([('patch_size', [1, 2, 2]),\n",
       "             ('num_attention_heads', 12),\n",
       "             ('attention_head_dim', 128),\n",
       "             ('in_channels', 16),\n",
       "             ('out_channels', 16),\n",
       "             ('text_dim', 4096),\n",
       "             ('freq_dim', 256),\n",
       "             ('ffn_dim', 8960),\n",
       "             ('num_layers', 30),\n",
       "             ('cross_attn_norm', True),\n",
       "             ('qk_norm', 'rms_norm_across_heads'),\n",
       "             ('eps', 1e-06),\n",
       "             ('image_dim', None),\n",
       "             ('added_kv_proj_dim', None),\n",
       "             ('rope_max_seq_len', 1024),\n",
       "             ('_use_default_values',\n",
       "              ['image_dim',\n",
       "               'num_layers',\n",
       "               'qk_norm',\n",
       "               'cross_attn_norm',\n",
       "               'num_attention_heads',\n",
       "               'patch_size',\n",
       "               'text_dim',\n",
       "               'ffn_dim',\n",
       "               'freq_dim',\n",
       "               'added_kv_proj_dim',\n",
       "               'attention_head_dim',\n",
       "               'eps',\n",
       "               'out_channels',\n",
       "               'rope_max_seq_len',\n",
       "               'in_channels']),\n",
       "             ('_class_name', 'WanTransformer3DModelPacked'),\n",
       "             ('_diffusers_version', '0.33.0.dev0'),\n",
       "             ('_name_or_path', 'Wan-AI/Wan2.1-T2V-1.3B-Diffusers')]),\n",
       " 'training': False,\n",
       " '_parameters': {'scale_shift_table': Parameter containing:\n",
       "  tensor([[[ 3.0857e-02, -5.1679e-02, -2.7867e-03,  ...,  1.7070e-02,\n",
       "            -1.2354e-04,  1.6341e-02],\n",
       "           [-1.8520e-01, -1.0265e-01, -1.3941e-01,  ..., -9.2882e-02,\n",
       "            -2.2060e-02,  1.7488e-03]]], requires_grad=True)},\n",
       " '_buffers': {},\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': {'rope': WanRotaryPosEmbed(),\n",
       "  'patch_embedding': Conv3d(16, 1536, kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
       "  'condition_embedder': WanTimeTextImageEmbedding(\n",
       "    (timesteps_proj): Timesteps()\n",
       "    (time_embedder): TimestepEmbedding(\n",
       "      (linear_1): Linear(in_features=256, out_features=1536, bias=True)\n",
       "      (act): SiLU()\n",
       "      (linear_2): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    )\n",
       "    (act_fn): SiLU()\n",
       "    (time_proj): Linear(in_features=1536, out_features=9216, bias=True)\n",
       "    (text_embedder): PixArtAlphaTextProjection(\n",
       "      (linear_1): Linear(in_features=4096, out_features=1536, bias=True)\n",
       "      (act_1): GELU(approximate='tanh')\n",
       "      (linear_2): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'blocks': ModuleList(\n",
       "    (0-29): 30 x WanTransformerBlock(\n",
       "      (norm1): FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn1): Attention(\n",
       "        (norm_q): RMSNorm()\n",
       "        (norm_k): RMSNorm()\n",
       "        (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (attn2): Attention(\n",
       "        (norm_q): RMSNorm()\n",
       "        (norm_k): RMSNorm()\n",
       "        (to_q): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_k): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_v): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_out): ModuleList(\n",
       "          (0): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (ffn): FeedForward(\n",
       "        (net): ModuleList(\n",
       "          (0): GELU(\n",
       "            (proj): Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          )\n",
       "          (1): Dropout(p=0.0, inplace=False)\n",
       "          (2): Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm3): FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "    )\n",
       "  ),\n",
       "  'norm_out': FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=False),\n",
       "  'proj_out': Linear(in_features=1536, out_features=64, bias=True)},\n",
       " '_gradient_checkpointing_func': None,\n",
       " 'gradient_checkpointing': False,\n",
       " 'inner_dim': 1536,\n",
       " 'in_channels': 16,\n",
       " 'clean_embedding': None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers_helper.memory import DynamicSwapInstaller\n",
    "\n",
    "DynamicSwapInstaller.install_model(transformer, device=gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_internal_dict': FrozenDict([('patch_size', [1, 2, 2]),\n",
       "             ('num_attention_heads', 12),\n",
       "             ('attention_head_dim', 128),\n",
       "             ('in_channels', 16),\n",
       "             ('out_channels', 16),\n",
       "             ('text_dim', 4096),\n",
       "             ('freq_dim', 256),\n",
       "             ('ffn_dim', 8960),\n",
       "             ('num_layers', 30),\n",
       "             ('cross_attn_norm', True),\n",
       "             ('qk_norm', 'rms_norm_across_heads'),\n",
       "             ('eps', 1e-06),\n",
       "             ('image_dim', None),\n",
       "             ('added_kv_proj_dim', None),\n",
       "             ('rope_max_seq_len', 1024),\n",
       "             ('_use_default_values',\n",
       "              ['image_dim',\n",
       "               'num_layers',\n",
       "               'qk_norm',\n",
       "               'cross_attn_norm',\n",
       "               'num_attention_heads',\n",
       "               'patch_size',\n",
       "               'text_dim',\n",
       "               'ffn_dim',\n",
       "               'freq_dim',\n",
       "               'added_kv_proj_dim',\n",
       "               'attention_head_dim',\n",
       "               'eps',\n",
       "               'out_channels',\n",
       "               'rope_max_seq_len',\n",
       "               'in_channels']),\n",
       "             ('_class_name', 'WanTransformer3DModelPacked'),\n",
       "             ('_diffusers_version', '0.33.0.dev0'),\n",
       "             ('_name_or_path', 'Wan-AI/Wan2.1-T2V-1.3B-Diffusers')]),\n",
       " 'training': False,\n",
       " '_parameters': {'scale_shift_table': Parameter containing:\n",
       "  tensor([[[ 3.0857e-02, -5.1679e-02, -2.7867e-03,  ...,  1.7070e-02,\n",
       "            -1.2354e-04,  1.6341e-02],\n",
       "           [-1.8520e-01, -1.0265e-01, -1.3941e-01,  ..., -9.2882e-02,\n",
       "            -2.2060e-02,  1.7488e-03]]], requires_grad=True)},\n",
       " '_buffers': {},\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_pre_hooks': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_hooks_with_kwargs': OrderedDict(),\n",
       " '_forward_hooks_always_called': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks_with_kwargs': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': {'rope': DynamicSwap_WanRotaryPosEmbed(),\n",
       "  'patch_embedding': DynamicSwap_Conv3d(16, 1536, kernel_size=(1, 2, 2), stride=(1, 2, 2)),\n",
       "  'condition_embedder': DynamicSwap_WanTimeTextImageEmbedding(\n",
       "    (timesteps_proj): DynamicSwap_Timesteps()\n",
       "    (time_embedder): DynamicSwap_TimestepEmbedding(\n",
       "      (linear_1): DynamicSwap_Linear(in_features=256, out_features=1536, bias=True)\n",
       "      (act): DynamicSwap_SiLU()\n",
       "      (linear_2): DynamicSwap_Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    )\n",
       "    (act_fn): DynamicSwap_SiLU()\n",
       "    (time_proj): DynamicSwap_Linear(in_features=1536, out_features=9216, bias=True)\n",
       "    (text_embedder): DynamicSwap_PixArtAlphaTextProjection(\n",
       "      (linear_1): DynamicSwap_Linear(in_features=4096, out_features=1536, bias=True)\n",
       "      (act_1): DynamicSwap_GELU(approximate='tanh')\n",
       "      (linear_2): DynamicSwap_Linear(in_features=1536, out_features=1536, bias=True)\n",
       "    )\n",
       "  ),\n",
       "  'blocks': DynamicSwap_ModuleList(\n",
       "    (0-29): 30 x DynamicSwap_WanTransformerBlock(\n",
       "      (norm1): DynamicSwap_FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "      (attn1): DynamicSwap_Attention(\n",
       "        (norm_q): DynamicSwap_RMSNorm()\n",
       "        (norm_k): DynamicSwap_RMSNorm()\n",
       "        (to_q): DynamicSwap_Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_k): DynamicSwap_Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_v): DynamicSwap_Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_out): DynamicSwap_ModuleList(\n",
       "          (0): DynamicSwap_Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (1): DynamicSwap_Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (attn2): DynamicSwap_Attention(\n",
       "        (norm_q): DynamicSwap_RMSNorm()\n",
       "        (norm_k): DynamicSwap_RMSNorm()\n",
       "        (to_q): DynamicSwap_Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_k): DynamicSwap_Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_v): DynamicSwap_Linear(in_features=1536, out_features=1536, bias=True)\n",
       "        (to_out): DynamicSwap_ModuleList(\n",
       "          (0): DynamicSwap_Linear(in_features=1536, out_features=1536, bias=True)\n",
       "          (1): DynamicSwap_Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm2): DynamicSwap_FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=True)\n",
       "      (ffn): DynamicSwap_FeedForward(\n",
       "        (net): DynamicSwap_ModuleList(\n",
       "          (0): DynamicSwap_GELU(\n",
       "            (proj): DynamicSwap_Linear(in_features=1536, out_features=8960, bias=True)\n",
       "          )\n",
       "          (1): DynamicSwap_Dropout(p=0.0, inplace=False)\n",
       "          (2): DynamicSwap_Linear(in_features=8960, out_features=1536, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm3): DynamicSwap_FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=False)\n",
       "    )\n",
       "  ),\n",
       "  'norm_out': DynamicSwap_FP32LayerNorm((1536,), eps=1e-06, elementwise_affine=False),\n",
       "  'proj_out': DynamicSwap_Linear(in_features=1536, out_features=64, bias=True)},\n",
       " '_gradient_checkpointing_func': None,\n",
       " 'gradient_checkpointing': False,\n",
       " 'inner_dim': 1536,\n",
       " 'in_channels': 16,\n",
       " 'clean_embedding': None,\n",
       " 'forge_backup_original_class': __main__.WanTransformer3DModelPacked}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.transformers.transformer_hunyuan_video import HunyuanVideoTransformer3DModelPacked"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FramePack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
